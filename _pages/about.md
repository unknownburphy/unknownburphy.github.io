---
layout: archive
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

# About

## Summary

I am a graduate student in Data Science at Seoul National University, researching value alignment in LLMs under [Prof. Yohan Jo](https://yohanjo.github.io/). Currently, I'm particularly interested in two areas: assessing values in LLMs and reasoning in LLMs. I believe that aligning language models with specific value systems is a crucial first step toward building personalized AI systems. I am also interested in improving reasoning capabilities in LLMs, particularly in mathematical domains.

## Research Interests

* Value alignment in LLMs
* Assessing values in LLMs
* Reasoning in LLMs

## Publications

**Value Portrait: Assessing Language Models' Values through Psychometrically and Ecologically Valid Items** [[arXiv](https://arxiv.org/abs/2505.01015)]  
Jongwook Han\*, <strong><em>Dongmin Choi</em></strong>\*, Woojung Song\*, Eun-Ju Lee, Yohan Jo
[ACL] Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, 2025, \*Equal Contributions

**PVP: An Image Dataset for Personalized Visual Persuasion with Persuasiveness Ratings, Persuasion Strategies, and Viewer Characteristics** [[arXiv](https://arxiv.org/abs/2506.00481)]  
Junseo Kim, Jongwook Han, ***Dongmin Choi***, Jongwook Yoon, Eun-Ju Lee, Yohan Jo  
[ACL] Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, 2025

### Preprints

**Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models** [[arXiv](https://arxiv.org/abs/2509.10078)]  
Dongmin Choi, Woojung Song, Jongwook Han, Eun-Ju Lee, Yohan Jo  
Under Review at TACL, 2025

**Future Policy Aware Preference Learning for Mathematical Reasoning** [[arXiv](https://arxiv.org/abs/2509.19893)]  
Minjae Oh, Yunho Choi, Dongmin Choi, Yohan Jo  
Under Review at ICLR 2026, 2025
